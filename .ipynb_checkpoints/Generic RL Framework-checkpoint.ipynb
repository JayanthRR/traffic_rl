{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskController:\n",
    "    def __init__(self, env, learning_algorithm='SARSA', exploration=True, exploration_decay='SIMULATED_ANNEALING', \n",
    "                 exploration_strategy='E_GREEDY', exploration_epsilon=0.8, \n",
    "                 learning_rate=0.01, learning_rate_decay='EXPONENTIAL', gamma=0.9, online_learning=False,\n",
    "                 *args, **kwargs):\n",
    "        self.env = env\n",
    "        policy = Policy(self.env, states=self.env.all_states(), actions=self.env.all_actions(),\n",
    "                        state_action_validity_checker=self.env.is_state_action_pair_valid, \n",
    "                        algorithm=learning_algorithm, exploration=exploration, exploration_decay=exploration_decay, \n",
    "                        exploration_strategy=exploration_strategy, exploration_epsilon=exploration_epsilon, \n",
    "                        learning_rate=learning_rate, learning_rate_decay=learning_rate_decay, gamma=gamma)\n",
    "        self.agent = Agent(policy, initial_state=self.env.get_initial_state())\n",
    "        self.online_learning = online_learning\n",
    "        \n",
    "    def run_episode(self, learning_phase=True):\n",
    "        \n",
    "        action = self.agent.choose_action(self.agent.current_state, always_greedy=not learning_phase)\n",
    "        \n",
    "        self.agent.next_state, reward = self.env.get_next_state_reward(self.agent.current_state, action)\n",
    "        if learning_phase:\n",
    "            if self.online_learning:\n",
    "                next_action = None\n",
    "                if self.agent.policy.algorithm_type == 'SARSA':\n",
    "                    next_action = self.agent.choose_action(self.agent.next_state, always_greedy=True)\n",
    "                elif self.agent.policy.algorithm_type == 'QLEARNING':\n",
    "                    next_action = None\n",
    "                else:\n",
    "                    # EXPECTED_SARSA all actions are averaged\n",
    "                    next_action = None\n",
    "                self.agent.feed_reward(reward, self.agent.current_state, self.agent.next_state, \n",
    "                                              action, next_action)\n",
    "            else:\n",
    "                self.agent.policy.history.append({'state': self.agent.current_state, 'action': action, 'reward': reward})\n",
    "                #print(self.agent.policy.history)\n",
    "        else:\n",
    "            self.agent.policy.history.append({'state': self.agent.current_state, 'action': action, 'reward': reward})\n",
    "                \n",
    "        if self.env.is_terminal(self.agent.next_state):\n",
    "            self.env.episode_terminated = True\n",
    "            if not self.online_learning and learning_phase:\n",
    "                self.agent.feed_reward(reward, self.agent.current_state, self.agent.next_state,\n",
    "                                                  None, None, online=False)\n",
    "                self.agent.reset_history()\n",
    "            \n",
    "        self.agent.current_state = self.agent.next_state\n",
    "        \n",
    "        return self.agent.current_state, action, self.agent.next_state, reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.episode_terminated = False\n",
    "        self.agent.current_state=(NUM_ROWS-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import random, choice\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, env, states={}, actions={}, state_action_validity_checker=None, \n",
    "                 hash_states=False, hash_actions=False,\n",
    "                 algorithm='QLEARNING', exploration=True, exploration_decay='SIMULATED_ANNEALING', \n",
    "                 exploration_strategy='E_GREEDY', exploration_epsilon=0.8, \n",
    "                 learning_rate=0.01, learning_rate_decay='EXPONENTIAL', gamma=0.9, *args, **kwargs):\n",
    "        # Set the update rule\n",
    "        self.env = env\n",
    "        self.algorithm_type = algorithm\n",
    "        \n",
    "        if algorithm=='QLEARNING':\n",
    "            self.algorithm = QLearning(learning_rate=learning_rate,\n",
    "                                       gamma=gamma, *args, **kwargs)\n",
    "        elif algorithm=='SARSA':\n",
    "            self.algorithm = Sarsa(learning_rate=learning_rate, \n",
    "                                   gamma=gamma,*args, **kwargs)\n",
    "        elif algorithm=='EXPECTED_SARSA':\n",
    "            self.algorithm = ExpectedSarsa(learning_rate=learning_rate, \n",
    "                                           gamma=gamma,*args, **kwargs)\n",
    "        elif algorithm=='EVERY_VISIT_MC':\n",
    "            self.algorithm = EveryVisitMC(*args, **kwargs)\n",
    "        elif algorithm=='FIRST_VISIT_MC':\n",
    "            self.algorithm = FirstVisitMC(*args, **kwargs)\n",
    "        else:\n",
    "            self.algorithm = QLearning(learning_rate=learning_rate,\n",
    "                                       gamma=gamma, *args, **kwargs)\n",
    "           \n",
    "        # Set exploration-exploitation strategy\n",
    "        self.exploration = exploration\n",
    "        if self.exploration:\n",
    "            self.exploration_decay = exploration_decay\n",
    "            self.exploration_strategy = exploration_strategy\n",
    "            self.exploration_epsilon = exploration_epsilon\n",
    "            \n",
    "        # Set LR and gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Initialize policy\n",
    "        self.states = [hash(state) for state in states] if hash_states else states\n",
    "        \n",
    "        self.hash_states = hash_states\n",
    "        self.hash_actions = hash_actions\n",
    "        \n",
    "        self.policy = dict()\n",
    "        actions = list(actions)\n",
    "        for state in self.states:\n",
    "            temp = dict()\n",
    "            for action in actions:\n",
    "                if state_action_validity_checker(state, action):\n",
    "                    temp[action] = 0.1\n",
    "            self.policy[state] = temp\n",
    "        \n",
    "        self.history = []\n",
    "    \n",
    "    def feed_reward(self, reward, current_state, next_state, current_action, next_action, online=True):\n",
    "        if online:\n",
    "            self.algorithm.feed_reward(self, reward=reward, \n",
    "                                       current_state=current_state, \n",
    "                                       next_state=next_state, \n",
    "                                       current_action=current_action, \n",
    "                                       next_action=next_action)\n",
    "        else:\n",
    "            # Offline episodic updates\n",
    "            # History is an array of dicts [{'state': state, 'action': action},{...},...]\n",
    "            last_element = True\n",
    "            \n",
    "            for elm in reversed(self.history):\n",
    "                reward = elm['reward']\n",
    "                if last_element:\n",
    "                    last_element = False\n",
    "                    next_state = elm['state']\n",
    "                    next_action = elm['action']\n",
    "                current_state = elm['state']\n",
    "                current_action = elm['action']\n",
    "                self.algorithm.feed_reward(self, reward=reward, \n",
    "                                           current_state=current_state, \n",
    "                                           next_state=next_state, \n",
    "                                           current_action=current_action, \n",
    "                                           next_action=next_action)\n",
    "                next_state = current_state\n",
    "                next_action = current_action\n",
    "                \n",
    "            if self.algorithm_type in ['EVERY_VISIT_MC', 'FIRST_VISIT_MC']:\n",
    "                for state, temp in self.algorithm.visit_count.items():\n",
    "                    for action, value in temp.items():\n",
    "                        self.policy[state][action] = (self.policy[state][action]*(self.algorithm.num_iter) \n",
    "                                                      + np.mean(self.algorithm.visit_count[state][action]))/(self.algorithm.num_iter+1)\n",
    "                self.algorithm.reset_episode()\n",
    "            \n",
    "        return reward\n",
    "    \n",
    "    def choose_action(self, state, always_greedy=False):\n",
    "        action = None\n",
    "        if always_greedy:\n",
    "            return max(self.policy[state], key=self.policy[state].get)\n",
    "        if self.exploration:\n",
    "            # Learning Phase\n",
    "            if random() > self.exploration_epsilon:\n",
    "                # Exploitation\n",
    "                action = max(self.policy[state], key=self.policy[state].get)\n",
    "            else:\n",
    "                # Exploration\n",
    "                if self.exploration_strategy == 'E_GREEDY':\n",
    "                    min_v = min(self.policy[state].values())\n",
    "                    non_neg_v = [v - min_v + 0.1 for v in self.policy[state].values()]\n",
    "                    total = sum(non_neg_v)\n",
    "                    ordered_actions = [(k,float(v - min_v + 0.1)/total) for k,v in self.policy[state].items()]\n",
    "                    a = [a[0] for a in ordered_actions]\n",
    "                    p = [a[1] for a in ordered_actions]\n",
    "                    action = choice(a, p=p)\n",
    "                else:\n",
    "                    action = choice(list(self.policy[state].keys()))\n",
    "                \n",
    "                # Decay of exploration rate\n",
    "                if self.exploration_decay=='SIMULATED_ANNEALING':\n",
    "                    self.exploration_epsilon = self.exploration_epsilon*0.99\n",
    "                elif self.exploration_decay=='CONSTANT':\n",
    "                    self.exploration_epsilon = self.exploration_epsilon\n",
    "        \n",
    "        else:\n",
    "            # Non Learning Phase\n",
    "            action = max(self.policy[state], key=self.policy[state].get)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    \n",
    "class Sarsa:\n",
    "    # Use >300 episodes\n",
    "    def __init__(self, learning_rate=0.01, gamma=0.9, eligibility_trace=False):\n",
    "        self.LR = learning_rate\n",
    "        self.GAMMA = gamma\n",
    "        self.num_iter = 0\n",
    "    \n",
    "    def feed_reward(self, policy, reward=None, current_state=None, next_state=None, current_action=None, next_action=None):\n",
    "        # Python is pass by name and mutability of object decides whether it acts as pass by value or reference\n",
    "        # Dicts are mutable and therefore this modifies the self.policy on Policy instance\n",
    "        policy.policy[current_state][current_action] = policy.policy[current_state][current_action] + self.LR*(\n",
    "            reward + self.GAMMA*policy.policy[next_state][next_action] - policy.policy[current_state][current_action])\n",
    "        \n",
    "        # Learning rate decay (logarithmic)\n",
    "        self.num_iter += 1\n",
    "        self.LR = min(1.0/np.log(self.num_iter), self.LR)\n",
    "        return self.num_iter\n",
    "    \n",
    "class QLearning:\n",
    "    # Use >300 episodes\n",
    "    def __init__(self, learning_rate=0.01, gamma=0.9, eligibility_trace=False):\n",
    "        self.LR = learning_rate\n",
    "        self.GAMMA = gamma\n",
    "        self.num_iter = 0\n",
    "    \n",
    "    def feed_reward(self, policy, reward=None, current_state=None, next_state=None, current_action=None, next_action=None):\n",
    "        # Python is pass by name and mutability of object decides whether it acts as pass by value or reference\n",
    "        # Dicts are mutable and therefore this modifies the self.policy on Policy instance\n",
    "        policy.policy[current_state][current_action] = policy.policy[current_state][current_action] + self.LR*(\n",
    "            reward + self.GAMMA*max(policy.policy[next_state].values()) - policy.policy[current_state][current_action])\n",
    "        \n",
    "        # Learning rate decay (logarithmic)\n",
    "        self.num_iter += 1\n",
    "        self.LR = min(1.0/np.log(self.num_iter), self.LR)\n",
    "        return self.num_iter\n",
    "    \n",
    "class ExpectedSarsa:\n",
    "    # Use >1000 episodes\n",
    "    def __init__(self, learning_rate=0.01, gamma=0.9, eligibility_trace=False):\n",
    "        self.LR = learning_rate\n",
    "        self.GAMMA = gamma\n",
    "        self.num_iter = 0\n",
    "    \n",
    "    def feed_reward(self, policy, reward=None, current_state=None, next_state=None, current_action=None, next_action=None):\n",
    "        # Python is pass by name and mutability of object decides whether it acts as pass by value or reference\n",
    "        # Dicts are mutable and therefore this modifies the self.policy on Policy instance\n",
    "        policy.policy[current_state][current_action] = policy.policy[current_state][current_action] + self.LR*(\n",
    "            reward + self.GAMMA*np.mean(list(policy.policy[next_state].values())) - policy.policy[current_state][current_action])\n",
    "        \n",
    "        # Learning rate decay (logarithmic)\n",
    "        self.num_iter += 1\n",
    "        self.LR = min(1.0/np.log(self.num_iter), self.LR)\n",
    "        return self.num_iter\n",
    "    \n",
    "    \n",
    "class EveryVisitMC:\n",
    "    # Use >1000 episodes\n",
    "    def __init__(self, gamma=0.9):\n",
    "        self.num_iter = 1\n",
    "        self.visit_count = dict()\n",
    "        self.episode_return = 0\n",
    "        self.GAMMA = gamma\n",
    "    \n",
    "    def feed_reward(self, policy, reward=None, current_state=None, next_state=None, current_action=None, next_action=None):\n",
    "        # Python is pass by name and mutability of object decides whether it acts as pass by value or reference\n",
    "        # Dicts are mutable and therefore this modifies the self.policy on Policy instance\n",
    "        self.episode_return += self.GAMMA*reward\n",
    "\n",
    "        if current_state in self.visit_count:\n",
    "            if current_action in self.visit_count[current_state]:\n",
    "                self.visit_count[current_state][current_action].append(self.episode_return)\n",
    "            else:\n",
    "                self.visit_count[current_state][current_action] = [self.episode_return]\n",
    "        else:\n",
    "            self.visit_count[current_state] = dict()\n",
    "            self.visit_count[current_state][current_action] = [self.episode_return]\n",
    "            \n",
    "        return self.num_iter\n",
    "    \n",
    "    def reset_episode(self):\n",
    "        self.visit_count = dict()\n",
    "        self.episode_return = 0\n",
    "        self.num_iter += 1\n",
    "\n",
    "    \n",
    "class FirstVisitMC:\n",
    "    # Use >1000 episodes\n",
    "    def __init__(self, gamma=0.9):\n",
    "        self.num_iter = 1\n",
    "        self.visit_count = dict()\n",
    "        self.episode_return = 0\n",
    "        self.GAMMA = gamma\n",
    "    \n",
    "    def feed_reward(self, policy, reward=None, current_state=None, next_state=None, current_action=None, next_action=None):\n",
    "        # Python is pass by name and mutability of object decides whether it acts as pass by value or reference\n",
    "        # Dicts are mutable and therefore this modifies the self.policy on Policy instance\n",
    "        self.episode_return += self.GAMMA*reward\n",
    "\n",
    "        if current_state in self.visit_count:\n",
    "            self.visit_count[current_state][current_action] = [self.episode_return]\n",
    "        else:\n",
    "            self.visit_count[current_state] = dict()\n",
    "            self.visit_count[current_state][current_action] = [self.episode_return]\n",
    "            \n",
    "        return self.num_iter\n",
    "    \n",
    "    def reset_episode(self):\n",
    "        self.visit_count = dict()\n",
    "        self.episode_return = 0\n",
    "        self.num_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, policy, initial_state=None):\n",
    "        self.current_state = initial_state\n",
    "        self.next_state = None\n",
    "        # Pass the generator object for both states and actions\n",
    "        self.policy = policy\n",
    "    \n",
    "    def reset_history(self):\n",
    "        self.policy.history = []\n",
    "        \n",
    "    def choose_action(self, *args, **kwargs):\n",
    "        return self.policy.choose_action(*args, **kwargs)\n",
    "    \n",
    "    def feed_reward(self, *args, **kwargs):\n",
    "        return self.policy.feed_reward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROWS = 4\n",
    "NUM_COLS = 10\n",
    "\n",
    "class CliffJump:\n",
    "    def __init__(self, num_rows=NUM_ROWS, num_cols=NUM_COLS):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.grid = np.zeros((self.num_rows,self.num_cols))\n",
    "        self.grid[self.num_rows-1,1:self.num_cols-1] = 1\n",
    "        self.episode_terminated = False\n",
    "    \n",
    "    def all_states(self):\n",
    "        for r in range(self.num_rows):\n",
    "            for c in range(self.num_cols):\n",
    "                yield (r,c)\n",
    "                \n",
    "    def all_actions(self):\n",
    "        for a in np.arange(4):\n",
    "            yield a\n",
    "    \n",
    "    def get_next_state_reward(self, current_state, action):\n",
    "        reward = 0\n",
    "        next_state = None\n",
    "        \n",
    "        if action==0:\n",
    "            next_state = (current_state[0]+1,current_state[1])\n",
    "        elif action==1:\n",
    "            next_state = (current_state[0],current_state[1]-1)\n",
    "        elif action==2:\n",
    "            next_state = (current_state[0]-1,current_state[1])\n",
    "        else:\n",
    "            next_state = (current_state[0],current_state[1]+1)\n",
    "            \n",
    "        if next_state[0]==self.num_rows-1 and (next_state[1] not in [0,self.num_cols-1]):\n",
    "            reward = -50\n",
    "        elif next_state[0]==self.num_rows-1 and next_state[1]==self.num_cols-1:\n",
    "            reward = 200\n",
    "        else:\n",
    "            reward = 0\n",
    "        return next_state, reward\n",
    "    \n",
    "    def is_state_action_pair_valid(self, state, action):\n",
    "        if (state[0]==self.num_rows-1 and action==0) or (state[0]==0 and action==2) or (state[1]==0 and action==1) or (state[1]==self.num_cols-1 and action==3):\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        if state[0]==self.num_rows-1 and state[1]!=0:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        return (self.num_rows-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaurav/Personal/Projects/cenv/lib/python3.6/site-packages/ipykernel_launcher.py:184: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-54 earned after 0 episodes\n",
      "-135 earned after 100 episodes\n",
      "-50 earned after 200 episodes\n",
      "-50 earned after 300 episodes\n",
      "-120 earned after 400 episodes\n",
      "-52 earned after 500 episodes\n",
      "-50 earned after 600 episodes\n",
      "-50 earned after 700 episodes\n",
      "-62 earned after 800 episodes\n",
      "-52 earned after 900 episodes\n"
     ]
    }
   ],
   "source": [
    "cliff_jump = CliffJump(num_rows=4, num_cols=10)\n",
    "\n",
    "tc = TaskController(cliff_jump, \n",
    "                    learning_algorithm='FIRST_VISIT_MC', # QLEARNING, SARSA, EXPECTED_SARSA, FIRST_VISIT_MC\n",
    "                    exploration=True, \n",
    "                    exploration_decay='CONSTANT', # CONSTANT, SIMULATED_ANNEALING\n",
    "                    exploration_strategy='SOFT_E_GREEDY', # E_GREEDY, SOFT_E_GREEDY\n",
    "                    exploration_epsilon=0.8, \n",
    "                    learning_rate=0.05, \n",
    "                    learning_rate_decay='EXPONENTIAL', # CONSTANT, EXPONENTIAL\n",
    "                    gamma=0.9,\n",
    "                    online_learning=True)\n",
    "\n",
    "NUM_EP=0\n",
    "MAX_EP=1000\n",
    "MIN_REWARD = -1000\n",
    "\n",
    "while NUM_EP < MAX_EP:\n",
    "    cum_reward = 0\n",
    "    while not tc.env.episode_terminated:\n",
    "        _, _, _, reward = tc.run_episode(learning_phase=True)\n",
    "        cum_reward += reward\n",
    "        if cum_reward < MIN_REWARD:\n",
    "            tc.reset()\n",
    "    if NUM_EP % (int(MAX_EP/10) if MAX_EP>10 else 1) == 0:\n",
    "        print(\"{} earned after {} episodes\".format(cum_reward,NUM_EP))\n",
    "    NUM_EP += 1\n",
    "    tc.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.reset()\n",
    "cum_reward = 0\n",
    "test_task = CliffJump(num_rows=4, num_cols=10)\n",
    "test_task.grid[3, 0]=2\n",
    "while not tc.env.episode_terminated:\n",
    "    _, action, next_state, reward = tc.run_episode(learning_phase=False)\n",
    "#     print(tc.agent.policy.policy[_], action, next_state)\n",
    "    test_task.grid[next_state[0], next_state[1]]=2\n",
    "    cum_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "# create discrete colormap\n",
    "cmap = colors.ListedColormap(['green', 'brown','blue'])\n",
    "bounds = [-0.5,0.5,1.5,2.5]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(test_task.grid, cmap=cmap, norm=norm)\n",
    "\n",
    "# draw gridlines\n",
    "ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=2)\n",
    "ax.set_xticks(np.arange(-0.5, 10, 1));\n",
    "ax.set_yticks(np.arange(-0.5, 4, 1));\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_task = CliffJump(num_rows=4, num_cols=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
