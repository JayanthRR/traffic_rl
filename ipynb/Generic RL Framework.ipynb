{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskController:\n",
    "    def __init__(self, env, learning_algorithm='SARSA', exploration=True, exploration_decay='SIMULATED_ANNEALING', \n",
    "                 exploration_strategy='E_GREEDY', exploration_epsilon=0.8, \n",
    "                 learning_rate=0.01, learning_rate_decay='EXPONENTIAL', gamma=0.9, online_learning=False,\n",
    "                 *args, **kwargs):\n",
    "        self.env = env\n",
    "        policy = Policy(self.env, states=self.env.all_states(), actions=self.env.all_actions(),\n",
    "                        state_action_validity_checker=self.env.is_state_action_pair_valid, \n",
    "                        algorithm=learning_algorithm, exploration=exploration, exploration_decay=exploration_decay, \n",
    "                        exploration_strategy=exploration_strategy, exploration_epsilon=exploration_epsilon, \n",
    "                        learning_rate=learning_rate, learning_rate_decay=learning_rate_decay, gamma=gamma)\n",
    "        self.agent = Agent(policy, initial_state=self.env.get_initial_state())\n",
    "        self.online_learning = online_learning\n",
    "        \n",
    "    def run_episode(self, learning_phase=True):\n",
    "        \n",
    "        action = self.agent.choose_action(self.agent.current_state, always_greedy=not learning_phase)\n",
    "        \n",
    "        self.agent.next_state, reward = self.env.get_next_state_reward(self.agent.current_state, action)\n",
    "        if learning_phase:\n",
    "            if self.online_learning:\n",
    "                next_action = None\n",
    "                if self.agent.policy.algorithm_type == 'SARSA':\n",
    "                    next_action = self.agent.choose_action(self.agent.next_state, always_greedy=True)\n",
    "                elif self.agent.policy.algorithm_type == 'QLEARNING':\n",
    "                    next_action = None\n",
    "                else:\n",
    "                    # EXPECTED_SARSA all actions are averaged\n",
    "                    next_action = None\n",
    "                self.agent.feed_reward(reward, self.agent.current_state, self.agent.next_state, \n",
    "                                              action, next_action)\n",
    "            else:\n",
    "                self.agent.policy.history.append({'state': self.agent.current_state, 'action': action, 'reward': reward})\n",
    "                #print(self.agent.policy.history)\n",
    "        else:\n",
    "            self.agent.policy.history.append({'state': self.agent.current_state, 'action': action, 'reward': reward})\n",
    "                \n",
    "        if self.env.is_terminal(self.agent.next_state):\n",
    "            self.env.episode_terminated = True\n",
    "            if not self.online_learning and learning_phase:\n",
    "                self.agent.feed_reward(reward, self.agent.current_state, self.agent.next_state,\n",
    "                                                  None, None, online=False)\n",
    "                self.agent.reset_history()\n",
    "            \n",
    "        self.agent.cur-rent_state = self.agent.next_state\n",
    "        \n",
    "        return self.agent.current_state, action, self.agent.next_state, reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.episode_terminated = False\n",
    "        self.agent.current_state=(NUM_ROWS-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import random, choice\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, env, states={}, actions={}, state_action_validity_checker=None, \n",
    "                 hash_states=False, hash_actions=False,\n",
    "                 algorithm='QLEARNING', exploration=True, exploration_decay='SIMULATED_ANNEALING', \n",
    "                 exploration_strategy='E_GREEDY', exploration_epsilon=0.8, \n",
    "                 learning_rate=0.01, learning_rate_decay='EXPONENTIAL', gamma=0.9, *args, **kwargs):\n",
    "        # Set the update rule\n",
    "        self.env = env\n",
    "        self.algorithm_type = algorithm\n",
    "        \n",
    "        if algorithm=='QLEARNING':\n",
    "            self.algorithm = QLearning(learning_rate=learning_rate,\n",
    "                                       gamma=gamma, *args, **kwargs)\n",
    "        elif algorithm=='SARSA':\n",
    "            self.algorithm = Sarsa(learning_rate=learning_rate, \n",
    "                                   gamma=gamma,*args, **kwargs)\n",
    "        elif algorithm=='EXPECTED_SARSA':\n",
    "            self.algorithm = ExpectedSarsa(learning_rate=learning_rate, \n",
    "                                           gamma=gamma,*args, **kwargs)\n",
    "        elif algorithm=='EVERY_VISIT_MC':\n",
    "            self.algorithm = EveryVisitMC(*args, **kwargs)\n",
    "        elif algorithm=='FIRST_VISIT_MC':\n",
    "            self.algorithm = FirstVisitMC(*args, **kwargs)\n",
    "        else:\n",
    "            self.algorithm = QLearning(learning_rate=learning_rate,\n",
    "                                       gamma=gamma, *args, **kwargs)\n",
    "           \n",
    "        # Set exploration-exploitation strategy\n",
    "        self.exploration = exploration\n",
    "        if self.exploration:\n",
    "            self.exploration_decay = exploration_decay\n",
    "            self.exploration_strategy = exploration_strategy\n",
    "            self.exploration_epsilon = exploration_epsilon\n",
    "            \n",
    "        # Set LR and gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Initialize policy\n",
    "        self.states = [hash(state) for state in states] if hash_states else states\n",
    "        \n",
    "        self.hash_states = hash_states\n",
    "        self.hash_actions = hash_actions\n",
    "        \n",
    "        self.policy = dict()\n",
    "        actions = list(actions)\n",
    "        for state in self.states:\n",
    "            temp = dict()\n",
    "            for action in actions:\n",
    "                if state_action_validity_checker(state, action):\n",
    "                    temp[action] = 0.1\n",
    "            self.policy[state] = temp\n",
    "        \n",
    "        self.history = []\n",
    "    \n",
    "    def feed_reward(self, reward, current_state, next_state, current_action, next_action, online=True):\n",
    "        if online:\n",
    "            self.algorithm.feed_reward(self, reward=reward, \n",
    "                                       current_state=current_state, \n",
    "                                       next_state=next_state, \n",
    "                                       current_action=current_action, \n",
    "                                       next_action=next_action)\n",
    "        else:\n",
    "            # Offline episodic updates\n",
    "            # History is an array of dicts [{'state': state, 'action': action},{...},...]\n",
    "            last_element = True\n",
    "            \n",
    "            for elm in reversed(self.history):\n",
    "                reward = elm['reward']\n",
    "                if last_element:\n",
    "                    last_element = False\n",
    "                    next_state = elm['state']\n",
    "                    next_action = elm['action']\n",
    "                current_state = elm['state']\n",
    "                current_action = elm['action']\n",
    "                self.algorithm.feed_reward(self, reward=reward, \n",
    "                                           current_state=current_state, \n",
    "                                           next_state=next_state, \n",
    "                                           current_action=current_action, \n",
    "                                           next_action=next_action)\n",
    "                next_state = current_state\n",
    "                next_action = current_action\n",
    "                \n",
    "            if self.algorithm_type in ['EVERY_VISIT_MC', 'FIRST_VISIT_MC']:\n",
    "                for state, temp in self.algorithm.visit_count.items():\n",
    "                    for action, value in temp.items():\n",
    "                        self.policy[state][action] = (self.policy[state][action]*(self.algorithm.num_iter) \n",
    "                                                      + np.mean(self.algorithm.visit_count[state][action]))/(self.algorithm.num_iter+1)\n",
    "                self.algorithm.reset_episode()\n",
    "            \n",
    "        return reward\n",
    "    \n",
    "    def choose_action(self, state, always_greedy=False):\n",
    "        action = None\n",
    "        if always_greedy:\n",
    "            return max(self.policy[state], key=self.policy[state].get)\n",
    "        if self.exploration:\n",
    "            # Learning Phase\n",
    "            if random() > self.exploration_epsilon:\n",
    "                # Exploitation\n",
    "                action = max(self.policy[state], key=self.policy[state].get)\n",
    "            else:\n",
    "                # Exploration\n",
    "                if self.exploration_strategy == 'E_GREEDY':\n",
    "                    min_v = min(self.policy[state].values())\n",
    "                    non_neg_v = [v - min_v + 0.1 for v in self.policy[state].values()]\n",
    "                    total = sum(non_neg_v)\n",
    "                    ordered_actions = [(k,float(v - min_v + 0.1)/total) for k,v in self.policy[state].items()]\n",
    "                    a = [a[0] for a in ordered_actions]\n",
    "                    p = [a[1] for a in ordered_actions]\n",
    "                    action = choice(a, p=p)\n",
    "                else:\n",
    "                    action = choice(list(self.policy[state].keys()))\n",
    "                \n",
    "                # Decay of exploration rate\n",
    "                if self.exploration_decay=='SIMULATED_ANNEALING':\n",
    "                    self.exploration_epsilon = self.exploration_epsilon*0.99\n",
    "                elif self.exploration_decay=='CONSTANT':\n",
    "                    self.exploration_epsilon = self.exploration_epsilon\n",
    "        \n",
    "        else:\n",
    "            # Non Learning Phase\n",
    "            action = max(self.policy[state], key=self.policy[state].get)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    \n",
    "class Sarsa:\n",
    "    # Use >300 episodes\n",
    "    def __init__(self, learning_rate=0.01, gamma=0.9, eligibility_trace=False):\n",
    "        self.LR = learning_rate\n",
    "        self.GAMMA = gamma\n",
    "        self.num_iter = 0\n",
    "    \n",
    "    def feed_reward(self, policy, reward=None, current_state=None, next_state=None, current_action=None, next_action=None):\n",
    "        # Python is pass by name and mutability of object decides whether it acts as pass by value or reference\n",
    "        # Dicts are mutable and therefore this modifies the self.policy on Policy instance\n",
    "        policy.policy[current_state][current_action] = policy.policy[current_state][current_action] + self.LR*(\n",
    "            reward + self.GAMMA*policy.policy[next_state][next_action] - policy.policy[current_state][current_action])\n",
    "        \n",
    "        # Learning rate decay (logarithmic)\n",
    "        self.num_iter += 1\n",
    "        self.LR = min(1.0/np.log(self.num_iter), self.LR)\n",
    "        return self.num_iter\n",
    "    \n",
    "class QLearning:\n",
    "    # Use >300 episodes\n",
    "    def __init__(self, learning_rate=0.01, gamma=0.9, eligibility_trace=False):\n",
    "        self.LR = learning_rate\n",
    "        self.GAMMA = gamma\n",
    "        self.num_iter = 0\n",
    "    \n",
    "    def feed_reward(self, policy, reward=None, current_state=None, next_state=None, current_action=None, next_action=None):\n",
    "        # Python is pass by name and mutability of object decides whether it acts as pass by value or reference\n",
    "        # Dicts are mutable and therefore this modifies the self.policy on Policy instance\n",
    "        policy.policy[current_state][current_action] = policy.policy[current_state][current_action] + self.LR*(\n",
    "            reward + self.GAMMA*max(policy.policy[next_state].values()) - policy.policy[current_state][current_action])\n",
    "        \n",
    "        # Learning rate decay (logarithmic)\n",
    "        self.num_iter += 1\n",
    "        self.LR = min(1.0/np.log(self.num_iter), self.LR)\n",
    "        return self.num_iter\n",
    "    \n",
    "class ExpectedSarsa:\n",
    "    # Use >1000 episodes\n",
    "    def __init__(self, learning_rate=0.01, gamma=0.9, eligibility_trace=False):\n",
    "        self.LR = learning_rate\n",
    "        self.GAMMA = gamma\n",
    "        self.num_iter = 0\n",
    "    \n",
    "    def feed_reward(self, policy, reward=None, current_state=None, next_state=None, current_action=None, next_action=None):\n",
    "        # Python is pass by name and mutability of object decides whether it acts as pass by value or reference\n",
    "        # Dicts are mutable and therefore this modifies the self.policy on Policy instance\n",
    "        policy.policy[current_state][current_action] = policy.policy[current_state][current_action] + self.LR*(\n",
    "            reward + self.GAMMA*np.mean(list(policy.policy[next_state].values())) - policy.policy[current_state][current_action])\n",
    "        \n",
    "        # Learning rate decay (logarithmic)\n",
    "        self.num_iter += 1\n",
    "        self.LR = min(1.0/np.log(self.num_iter), self.LR)\n",
    "        return self.num_iter\n",
    "    \n",
    "    \n",
    "class EveryVisitMC:\n",
    "    # Use >1000 episodes\n",
    "    def __init__(self, gamma=0.9):\n",
    "        self.num_iter = 1\n",
    "        self.visit_count = dict()\n",
    "        self.episode_return = 0\n",
    "        self.GAMMA = gamma\n",
    "    \n",
    "    def feed_reward(self, policy, reward=None, current_state=None, next_state=None, current_action=None, next_action=None):\n",
    "        # Python is pass by name and mutability of object decides whether it acts as pass by value or reference\n",
    "        # Dicts are mutable and therefore this modifies the self.policy on Policy instance\n",
    "        self.episode_return += self.GAMMA*reward\n",
    "\n",
    "        if current_state in self.visit_count:\n",
    "            if current_action in self.visit_count[current_state]:\n",
    "                self.visit_count[current_state][current_action].append(self.episode_return)\n",
    "            else:\n",
    "                self.visit_count[current_state][current_action] = [self.episode_return]\n",
    "        else:\n",
    "            self.visit_count[current_state] = dict()\n",
    "            self.visit_count[current_state][current_action] = [self.episode_return]\n",
    "            \n",
    "        return self.num_iter\n",
    "    \n",
    "    def reset_episode(self):\n",
    "        self.visit_count = dict()\n",
    "        self.episode_return = 0\n",
    "        self.num_iter += 1\n",
    "\n",
    "    \n",
    "class FirstVisitMC:\n",
    "    # Use >1000 episodes\n",
    "    def __init__(self, gamma=0.9):\n",
    "        self.num_iter = 1\n",
    "        self.visit_count = dict()\n",
    "        self.episode_return = 0\n",
    "        self.GAMMA = gamma\n",
    "    \n",
    "    def feed_reward(self, policy, reward=None, current_state=None, next_state=None, current_action=None, next_action=None):\n",
    "        # Python is pass by name and mutability of object decides whether it acts as pass by value or reference\n",
    "        # Dicts are mutable and therefore this modifies the self.policy on Policy instance\n",
    "        self.episode_return += self.GAMMA*reward\n",
    "\n",
    "        if current_state in self.visit_count:\n",
    "            self.visit_count[current_state][current_action] = [self.episode_return]\n",
    "        else:\n",
    "            self.visit_count[current_state] = dict()\n",
    "            self.visit_count[current_state][current_action] = [self.episode_return]\n",
    "            \n",
    "        return self.num_iter\n",
    "    \n",
    "    def reset_episode(self):\n",
    "        self.visit_count = dict()\n",
    "        self.episode_return = 0\n",
    "        self.num_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, policy, initial_state=None):\n",
    "        self.current_state = initial_state\n",
    "        self.next_state = None\n",
    "        # Pass the generator object for both states and actions\n",
    "        self.policy = policy\n",
    "    \n",
    "    def reset_history(self):\n",
    "        self.policy.history = []\n",
    "        \n",
    "    def choose_action(self, *args, **kwargs):\n",
    "        return self.policy.choose_action(*args, **kwargs)\n",
    "    \n",
    "    def feed_reward(self, *args, **kwargs):\n",
    "        return self.policy.feed_reward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ROWS = 4\n",
    "NUM_COLS = 10\n",
    "\n",
    "class CliffJump:\n",
    "    def __init__(self, num_rows=NUM_ROWS, num_cols=NUM_COLS):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.grid = np.zeros((self.num_rows,self.num_cols))\n",
    "        self.grid[self.num_rows-1,1:self.num_cols-1] = 1\n",
    "        self.episode_terminated = False\n",
    "    \n",
    "    def all_states(self):\n",
    "        for r in range(self.num_rows):\n",
    "            for c in range(self.num_cols):\n",
    "                yield (r,c)\n",
    "                \n",
    "    def all_actions(self):\n",
    "        for a in np.arange(4):\n",
    "            yield a\n",
    "    \n",
    "    def get_next_state_reward(self, current_state, action):\n",
    "        reward = 0\n",
    "        next_state = None\n",
    "        \n",
    "        if action==0:\n",
    "            next_state = (current_state[0]+1,current_state[1])\n",
    "        elif action==1:\n",
    "            next_state = (current_state[0],current_state[1]-1)\n",
    "        elif action==2:\n",
    "            next_state = (current_state[0]-1,current_state[1])\n",
    "        else:\n",
    "            next_state = (current_state[0],current_state[1]+1)\n",
    "            \n",
    "        if next_state[0]==self.num_rows-1 and (next_state[1] not in [0,self.num_cols-1]):\n",
    "            reward = -50\n",
    "        elif next_state[0]==self.num_rows-1 and next_state[1]==self.num_cols-1:\n",
    "            reward = 200\n",
    "        else:\n",
    "            reward = 0\n",
    "        return next_state, reward\n",
    "    \n",
    "    def is_state_action_pair_valid(self, state, action):\n",
    "        if (state[0]==self.num_rows-1 and action==0) or (state[0]==0 and action==2) or (state[1]==0 and action==1) or (state[1]==self.num_cols-1 and action==3):\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        if state[0]==self.num_rows-1 and state[1]!=0:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        return (self.num_rows-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-50 earned after 0 episodes\n",
      "-50 earned after 100 episodes\n",
      "-50 earned after 200 episodes\n",
      "-50 earned after 300 episodes\n",
      "-50 earned after 400 episodes\n",
      "-50 earned after 500 episodes\n",
      "-50 earned after 600 episodes\n",
      "-50 earned after 700 episodes\n",
      "-50 earned after 800 episodes\n",
      "-50 earned after 900 episodes\n"
     ]
    }
   ],
   "source": [
    "cliff_jump = CliffJump(num_rows=4, num_cols=10)\n",
    "\n",
    "tc = TaskController(cliff_jump, \n",
    "                    learning_algorithm='EVERY_VISIT_MC',#'FIRST_VISIT_MC', # QLEARNING, SARSA, EXPECTED_SARSA, FIRST_VISIT_MC\n",
    "                    exploration=True, \n",
    "                    exploration_decay='CONSTANT', # CONSTANT, SIMULATED_ANNEALING\n",
    "                    exploration_strategy='SOFT_E_GREEDY', # E_GREEDY, SOFT_E_GREEDY\n",
    "                    exploration_epsilon=0.8, \n",
    "                    learning_rate=0.05, \n",
    "                    learning_rate_decay='EXPONENTIAL', # CONSTANT, EXPONENTIAL\n",
    "                    gamma=0.9,\n",
    "                    online_learning=True)\n",
    "\n",
    "NUM_EP=0\n",
    "MAX_EP=1000\n",
    "MIN_REWARD = -1000\n",
    "\n",
    "while NUM_EP < MAX_EP:\n",
    "    cum_reward = 0\n",
    "    while not tc.env.episode_terminated:\n",
    "        _, _, _, reward = tc.run_episode(learning_phase=True)\n",
    "        cum_reward += reward\n",
    "        if cum_reward < MIN_REWARD:\n",
    "            tc.reset()\n",
    "    if NUM_EP % (int(MAX_EP/10) if MAX_EP>10 else 1) == 0:\n",
    "        print(\"{} earned after {} episodes\".format(cum_reward,NUM_EP))\n",
    "    NUM_EP += 1\n",
    "    tc.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2e81871a6415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_terminated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#     print(tc.agent.policy.policy[_], action, next_state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtest_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-7f7279283c50>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(self, learning_phase)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_phase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_greedy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mlearning_phase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_state_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7c09e228d3cc>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3af117cf192e>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, state, always_greedy)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0malways_greedy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# Learning Phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "tc.reset()\n",
    "cum_reward = 0\n",
    "test_task = CliffJump(num_rows=4, num_cols=10)\n",
    "test_task.grid[3, 0]=2\n",
    "while not tc.env.episode_terminated:\n",
    "    _, action, next_state, reward = tc.run_episode(learning_phase=False)\n",
    "#     print(tc.agent.policy.policy[_], action, next_state)\n",
    "    test_task.grid[next_state[0], next_state[1]]=2\n",
    "    cum_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAACpCAYAAAAr+EqPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADcFJREFUeJzt3H2sZHV9x/H3x91FUFBWFmHDg0AlRKR/qBsqIZoNlNZQg0Zpi6mWbSSLtijVmsraZL21aaXGGE2wRYIUmirVQJUthfIQqGgaKHdxEXaRghZlFcJTwW4g2JVv/5gfOp3OvcvunHlo7vuVTOacOb853++dObufOWfOnFQVkiS9aNoNSJJmg4EgSQIMBElSYyBIkgADQZLUGAiSJGDEQEjyiiQ3JLmv3a9cYNzPkmxpt02j1JQkjUdG+R1Ckk8BT1TV+UnOA1ZW1UeHjNtRVfuO0KckacxGDYR7gbVV9VCS1cC/VNUxQ8YZCJI040b9DuGgqnqoTT8MHLTAuL2TzCe5NcnbR6wpSRqD5bsakORG4OAhi/6kf6aqKslCuxuvqqofJTkKuCnJXVX1vSG11gPr2+wbdtWbJOn/eKyqDtyTJ07kkNHAcy4Frq6qK3Yxrpjb49a6MTdwv1R76K8/t8iYpdBDf/25RcYshR76688tMmYp9NBff26RMZPpYXNVrdmTp496yGgTcGabPhO4anBAkpVJXtymVwEnAttGrCtJ6tiogXA+cEqS+4BfbfMkWZPk4jbmNcB8kjuBm4Hzq8pAkKQZs8vvEBZTVY8DJw95fB44q03/K/DLo9SRJI2fv1SWJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEdBUKStyS5N8n9Sc4bsnxdkkeTbGm3s7qoK0nqzvJRV5BkGfB54BRgO3B7kk1VtW1g6Feq6pxR60mSxiNVNdoKkhOAuar69Ta/AaCqPtk3Zh2wZncCIclojUnS0rS5qtbsyRO7OGR0CPBg3/z29tigdyb5TpIrkhw2bEVJ1ieZTzLfQV+SpN0w8iGjF+gfgcur6tkkZwOXAScNDqqqi4CLoO0hzE2ou4XMDdwv1R76688tMmYp9NBff26RMUuhh/76c4uMWQo99NefW2TMpHrYQ13sIfwI6P/Ef2h77Oeq6vGqerbNXgy8oYO6kqQOdREItwNHJzkyyV7AGcCm/gFJVvfNngbc00FdSVKHRj5kVFU7k5wDXAcsAy6pqq1JPgHMV9Um4INJTgN2Ak8A60atK0nqViffIVTVNcA1A49t7JveAGzoopYkaTz8pbIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1nQRCkkuSPJLk7gWWr03yVJIt7baxi7qSpO4s72g9lwIXAH+7yJhvVtVbO6onSepYJ3sIVXUL8EQX65IkTUeqqpsVJUcAV1fVcUOWrQWuBLYDPwY+UlVbd7G+bhqTpKVlc1Wt2ZMndnXIaFfuAF5VVTuSnAp8HTh6cFCS9cD6CfUkSeozkT2EIWMfANZU1WOLjCmY9k5CendzU2xhbuB+WuYG7pdqD/315xYZsxR66K8/t8iYpdBDf/25RcZMpoc93kOYyGmnSQ5OkjZ9fKv7+CRqS5JemE4OGSW5HFgLrEqyHfg4sAKgqi4ETgfen2Qn8AxwRnW1ayJJ6kQngVBV79rF8gvonZYqSZpR/lJZkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgR0EAhJDktyc5JtSbYmOXfImLVJnkqypd02jlpXktSt5R2sYyfwR1V1R5L9gM1JbqiqbQPjvllVb+2gniRpDFJV3a4wuQq4oKpu6HtsLfCR3QmEJN02JklLw+aqWrMnT+z0O4QkRwCvA24bsviEJHcmuTbJaxd4/vok80nmu+xLkrRrne0hJNkX+Abw51X1DwPLXgY8V1U7kpwKfK6qjt7F+gqmvZMQAL507LFT6+B3tm2beg+z0scs9DArfcxCD7PSxyz00N8Hc1Nsold7unsISVYAVwJfGgwDgKr6SVXtaNPXACuSrOqitiSpG12cZRTgi8A9VfWZBcYc3MaR5PhW9/FRa0uSutPFWUYnAu8B7kqypT32MeBwgKq6EDgdeH+SncAzwBnV9bfZkqSRjBwIVfUtnj/YvvCYC4ALRq0lSRoff6ksSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJzciBkGTvJP+W5M4kW5P86ZAx65I8mmRLu501al1JUreWd7COZ4GTqmpHkhXAt5JcW1W3Doz7SlWd00E9SdIYjBwIVVXAjja7ot1q1PVKkiYrvf/PR1xJsgzYDLwa+HxVfXRg+Trgk8CjwL8DH6qqB3exTkNFknbf5qpasydP7CQQfr6yZH/ga8AHquruvscPAHZU1bNJzgZ+u6pOGvL89cD6NnsccPfgmClYBTxmD8Bs9DELPcBs9DELPcBs9DELPcBs9HFMVe23J0/sNBAAkmwEnq6qTy+wfBnwRFW9fBfrmd/TlOvSLPQxCz3MSh+z0MOs9DELPcxKH7PQw6z0MUoPXZxldGDbMyDJPsApwHcHxqzumz0NuGfUupKkbnVxltFq4LL2yf9FwFer6uoknwDmq2oT8MEkpwE7gSeAdR3UlSR1qIuzjL4DvG7I4xv7pjcAG3Zz1ReN2FpXZqGPWegBZqOPWegBZqOPWegBZqOPWegBZqOPPe6h8+8QJEn/P3npCkkSMEOBkOQVSW5Icl+7X7nAuJ/1XQJjU4f135Lk3iT3JzlvyPKxX34jySVJHkky9HTbJGuTPNXXw8Zh40bs4bAkNyfZ1i5Fcu6k+5ily6EkWZbk20munmIPDyS5q9WYH7J87NtFq7N/kiuSfDfJPUlOmGQfSY7pW/eWJD9J8oeT7KHV+FDbLu9OcnmSvQeWT2q7OLf1sHXwdWjLd/+1qKqZuAGfAs5r0+cBf7nAuB1jqL0M+B5wFLAXcCdw7MCYdcAFY34N3gy8Hrh7geVrgavH3MNq4PVtej96PyQcfC3G2gcQYN82vQK4DXjjpN+PVufDwJeH/b0T7OEBYNUiy8e+XbQ6lwFntem9gP2n0UertQx4GHjVJHsADgH+A9inzX8VWDfp7YJf/E7rJfS+C74RePWor8XM7CEAb6O3wdHu3z7B2scD91fV96vqp8Dft34mqqpuoXcW1tRU1UNVdUeb/i96pwgfMuEeqqqmfjmUJIcCvwFcPOnasybJy+l9YPkiQFX9tKqenGJLJwPfq6ofTKH2cmCfJMvp/Yf84yn08Brgtqp6uqp2At8A3jHqSmcpEA6qqofa9MPAQQuM2zvJfJJbk3QVGocA/ZfS2M7w/wTfmeQ7bbf5sI5q764T2qGUa5O8dpyFkhxB7wyy2ybdRztUswV4BLihqob1MO7347PAHwPPLTJmEttEAdcn2Zzer/mHGfd2cSS9S8/8TTuEdnGSl06hj+edAVy+wLKx9VBVPwI+DfwQeAh4qqquHzJ03NvF3cCbkhyQ5CXAqcCwOrv3Wkxi965vF+bG9ocM3t4GPDkw9j8X2mVr90fR25X+pQ76Oh24uG/+PQzs8gEHAC9u02cDN43pNTqChQ8ZvYxfHEo5FbhvjO/VvvSuT/WOKfexP3AzcNwk3w/grcBftem1DD9kNKlt4vlt/pX0Dme+edLvB7CG3u+IfqXNfw74s2lsF/QOVz1G70PkRLdNYCVwE3AgvT3XrwPvntJ28d72b/QW4K+Bz476WnTe5Ah/3L3A6ja9Grj3BTznUuD0DmqfAFzXN78B2LDI+GX0PhmM43VYMBCGjH2ARY4tj9DDCuA64MPT7KNv/RuBj0zy/aB3Mcbt7W97GHga+LtpbBMDdeYWey3G9X4ABwMP9M2/CfinaWwX9D5AXv8Cx3baA/CbwBf75n+X9sFhytvFXwC/P+prMUuHjDYBZ7bpM4GrBgckWZnkxW16FXAisK2D2rcDRyc5Msle9HZH/9cZTJmBy28kOThJ2vTx9A75Pd5xjdA7TnxPVX1mGn1kBi6HUlUbqurQqjqC3vZwU1W9e5I9tBovTbLf89PArzFw0cdJbBdV9TDwYJJj2kMnM/BvbxJ9NO9igcNFE+jhh8Abk7yk1TmZgfd9Uv9XJHlluz+c3vcHXx5YvtuvRReXrujK+cBXk7wX+AHwWwBJ1gDvq6qz6H2R8oUkz9H7486vqpEDoap2JjmH3qfiZcAlVbU1E778RpLL6R2eWJVkO/Bxep/WqaoL6R3aen+SncAzwBnVor9DJ9I7ZHZXO4YP8DHg8An2MbOXQ5lCDwcBX2v/rpcDX66qf07yPpjodgHwAeBL7UPT94Hfm3QfLRRPoXco5vnHJtZDVd2W5ArgDnrv+7eBi6a0bV6Z3pWk/xv4g6p6ctTXwl8qS5KA2TrLSJI0RQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJAD+B2Swcvnn09jAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "# create discrete colormap\n",
    "cmap = colors.ListedColormap(['green', 'brown','blue'])\n",
    "bounds = [-0.5,0.5,1.5,2.5]\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(test_task.grid, cmap=cmap, norm=norm)\n",
    "\n",
    "# draw gridlines\n",
    "ax.grid(which='major', axis='both', linestyle='-', color='k', linewidth=2)\n",
    "ax.set_xticks(np.arange(-0.5, 10, 1));\n",
    "ax.set_yticks(np.arange(-0.5, 4, 1));\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_task = CliffJump(num_rows=4, num_cols=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
